{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526d970f",
   "metadata": {},
   "source": [
    "# Traffic signs detection project #\n",
    "Section 3: CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b696295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "path_base = os.getcwd() + \"\\\\databases\"\n",
    "train, test = np.load(os.path.join(path_base,'train.npy')), np.load(os.path.join(path_base,'test.npy'))\n",
    "# train = train.astype(\"float32\") / 255\n",
    "# train = np.expand_dims(train, -1)\n",
    "# test = test.astype(\"float32\") / 255\n",
    "# test = np.expand_dims(test, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09e1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pict = os.getcwd() + \"\\\\pictures\"\n",
    "\n",
    "f = open(path_pict + \"\\\\TsignRecgTrain4170Annotation.txt\", \"r\")\n",
    "str = f.read()\n",
    "\n",
    "lines = str.split(\"\\n\")\n",
    "arr = []\n",
    "for line in lines:\n",
    "    arr.append(int(line.split(\";\")[7]))\n",
    "    \n",
    "Train_classes = []\n",
    "for i in range(58):\n",
    "    for k in range(arr.count(i)):\n",
    "        Train_classes.append(i)\n",
    "\n",
    "#get classification from file\n",
    "path_pict = os.getcwd() + \"\\\\pictures\"\n",
    "\n",
    "f = open(path_pict + \"\\\\TsignRecgTest1994Annotation.txt\", \"r\")\n",
    "str = f.read()\n",
    "\n",
    "lines = str.split(\"\\n\")\n",
    "arr = []\n",
    "for line in lines:\n",
    "    arr.append(int(line.split(\";\")[7]))\n",
    "    \n",
    "Test_classes = []\n",
    "for i in range(58):\n",
    "    for k in range(arr.count(i)):\n",
    "        Test_classes.append(i)\n",
    "Test_classes = np.array(Test_classes)\n",
    "Train_classes = np.array(Train_classes)\n",
    "\n",
    "large_train_classes = []\n",
    "for i in range(0, 4170*3):\n",
    "    large_train_classes.append(Train_classes[i % 4170])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18d52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we augmentate the images in the array in order to get a larger data set and try to reach higher accuracy percentage:\n",
    "from scipy import ndimage\n",
    "\n",
    "large_train = []\n",
    "\n",
    "for i in range(0, 4170):\n",
    "    large_train.append(train[i])\n",
    "for i in range(0, 4170):\n",
    "    large_train.append(ndimage.rotate(train[i], 20, reshape=False))\n",
    "for i in range(0, 4170):\n",
    "    large_train.append(ndimage.rotate(train[i], -20, reshape=False))\n",
    "\n",
    "large_train = np.array(large_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fb525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343cef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 45, 45, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 40, 40, 24)        888       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 40, 40, 24)       96        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 18, 18, 48)        28848     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 18, 18, 48)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          49216     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 58)                237626    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 58)                0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 58)               232       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 58)                3422      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,584\n",
      "Trainable params: 320,292\n",
      "Non-trainable params: 292\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "88/88 [==============================] - 95s 1s/step - loss: 2.6594 - accuracy: 0.4220 - val_loss: 3.2522 - val_accuracy: 0.2622\n",
      "Epoch 2/15\n",
      "88/88 [==============================] - 102s 1s/step - loss: 1.4594 - accuracy: 0.6922 - val_loss: 3.2837 - val_accuracy: 0.2654\n",
      "Epoch 3/15\n",
      "88/88 [==============================] - 104s 1s/step - loss: 0.9986 - accuracy: 0.7815 - val_loss: 3.0282 - val_accuracy: 0.3197\n",
      "Epoch 4/15\n",
      "88/88 [==============================] - 100s 1s/step - loss: 0.7336 - accuracy: 0.8392 - val_loss: 2.7595 - val_accuracy: 0.3205\n",
      "Epoch 5/15\n",
      "88/88 [==============================] - 88s 1s/step - loss: 0.5724 - accuracy: 0.8723 - val_loss: 2.4196 - val_accuracy: 0.3309\n",
      "Epoch 6/15\n",
      "88/88 [==============================] - 105s 1s/step - loss: 0.4353 - accuracy: 0.9067 - val_loss: 2.1799 - val_accuracy: 0.3901\n",
      "Epoch 7/15\n",
      "88/88 [==============================] - 112s 1s/step - loss: 0.3693 - accuracy: 0.9183 - val_loss: 1.8752 - val_accuracy: 0.4373\n",
      "Epoch 8/15\n",
      "88/88 [==============================] - 72s 814ms/step - loss: 0.3013 - accuracy: 0.9358 - val_loss: 1.8153 - val_accuracy: 0.5012\n",
      "Epoch 9/15\n",
      "88/88 [==============================] - 166s 2s/step - loss: 0.2449 - accuracy: 0.9467 - val_loss: 1.7169 - val_accuracy: 0.5212\n",
      "Epoch 10/15\n",
      "88/88 [==============================] - 84s 956ms/step - loss: 0.2154 - accuracy: 0.9526 - val_loss: 1.5461 - val_accuracy: 0.5675\n",
      "Epoch 11/15\n",
      "88/88 [==============================] - 60s 678ms/step - loss: 0.1898 - accuracy: 0.9570 - val_loss: 1.5450 - val_accuracy: 0.5508\n",
      "Epoch 12/15\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.1629 - accuracy: 0.9640 - val_loss: 1.4168 - val_accuracy: 0.5891\n",
      "Epoch 13/15\n",
      "88/88 [==============================] - 58s 662ms/step - loss: 0.1464 - accuracy: 0.9684 - val_loss: 1.7354 - val_accuracy: 0.5667\n",
      "Epoch 14/15\n",
      "88/88 [==============================] - 33s 371ms/step - loss: 0.1290 - accuracy: 0.9726 - val_loss: 1.4284 - val_accuracy: 0.5787\n",
      "Epoch 15/15\n",
      " 3/88 [>.............................] - ETA: 34s - loss: 0.1010 - accuracy: 0.9766"
     ]
    }
   ],
   "source": [
    "# Neural Network (CNN):\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 58\n",
    "input_shape = (45, 45, 1)\n",
    "y_train = keras.utils.to_categorical(large_train_classes, num_classes)\n",
    "y_test = keras.utils.to_categorical(Test_classes, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Rescaling(scale=1 / 127.5, offset=-1),\n",
    "        layers.Conv2D(24, kernel_size=(6, 6), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(48, kernel_size=(5, 5), strides=(2, 2), activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(large_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00939b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    " \n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs, accuracy, 'g', label='Training accuracy')\n",
    "ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
    "ax[0].legend()\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs, loss_values, 'g', label='Training loss')\n",
    "ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e544e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
